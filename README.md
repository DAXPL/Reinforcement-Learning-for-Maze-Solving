# Reinforcement-Learning for Maze Solving
![Carrot](gfx/carrot.png)
<br />
Projekt ten stanowi implementacjÄ™ agenta uczenia ze wzmacnianiem, ktÃ³ry zdobywa umiejÄ™tnoÅ›Ä‡ rozwiÄ…zywania labiryntu przy uÅ¼yciu algorytmu Q-learning. Dodatkowo, zawiera rÃ³wnieÅ¼ generator labiryntÃ³w, ktÃ³ry tworzy zrÃ³Å¼nicowane trasy do eksploracji przez agenta.
<br />
Projekt zaliczeniowy zajÄ™Ä‡ laboratoryjnych przedmiotu "FizykaÂ komputerowa", na V semestrze Technologii Komputerowych, wydziaÅ‚u fizyki, Uniwersytetu im. Adama Mickiewicza w Poznaniu. Temat nr. 26 "Uczenie ze wzmacnianiem. Labirynt. (PrzykÅ‚ad: https://strikingloo.github.io/reinforcement-learning-beginners) "
<br />
<br />
W dzisiejszym Å›wiecie, gdzie sztuczna inteligencja i automatyka stajÄ… siÄ™ coraz bardziej obecne, zrozumienie i implementacja algorytmÃ³w uczenia ze wzmacnianiem staje siÄ™ kluczowe. Projekt ten ma na celu nie tylko eksploracjÄ™ i zrozumienie tych koncepcji, ale takÅ¼e praktyczne ich zastosowanie w rozwiÄ…zaniu konkretnego problemu, jakim jest przejÅ›cie labiryntu.
GÅ‚Ã³wnym celem projektu jest zaimplementowanie agenta zdolnego do samodzielnego rozwiÄ…zywania labiryntu przy uÅ¼yciu algorytmu Q-learn. Ponadto, projekt obejmuje stworzenie generatora labiryntÃ³w, ktÃ³ry pozwoli na tworzenie rÃ³Å¼norodnych Å›rodowisk do testowania zdolnoÅ›ci agenta.

## ğŸš€ Czym Jest Reinforcement Learning?

Reinforcement Learning, czyli uczenie ze wzmacnianiem, to koncepcja z dziedziny sztucznej inteligencji, gdzie agent uczony jest podejmowaÄ‡ decyzje w dynamicznym Å›rodowisku. Kluczowym elementem jest to, Å¼e agent zdobywa doÅ›wiadczenie, interakcjonujÄ…c z otoczeniem, i otrzymuje informacjÄ™ zwrotnÄ… w postaci nagrÃ³d lub kar.

### ğŸ‘¾ Kluczowe Elementy RL:

- **Agent:** To podmiot, ktÃ³ry podejmuje decyzje w Å›rodowisku, starajÄ…c siÄ™ maksymalizowaÄ‡ sumÄ™ nagrÃ³d.

- **Åšrodowisko:** To kontekst, w ktÃ³rym agent dziaÅ‚a. Åšrodowisko reaguje na decyzje agenta, dostarczajÄ…c nowych stanÃ³w i nagrÃ³d.

- **Akcje:** SÄ… to decyzje podejmowane przez agenta w danym stanie Å›rodowiska.

- **Stany:** ReprezentujÄ… okreÅ›lony kontekst lub sytuacjÄ™ w Å›rodowisku.

- **Nagrody i kary:** SÄ… uÅ¼ywane do oceny decyzji agenta. Cel agenta to maksymalizacja Å‚Ä…cznej zdyskontowanej sumy nagrÃ³d.

- **Funkcja WartoÅ›ci:** Ocenia, jak dobre sÄ… rÃ³Å¼ne stany lub akcje w danym kontekÅ›cie.

- **Eksploracja a Eksploatacja** Agent musi zbalansowaÄ‡ eksploracjÄ™ nowych dziaÅ‚aÅ„ i eksploatacjÄ™ juÅ¼ znanego, aby osiÄ…gnÄ…Ä‡ optymalne wyniki.

### ğŸ” Dlaczego To WaÅ¼ne?

Reinforcement Learning ma szerokie zastosowanie, od sterowania robotami po automatyczne podejmowanie decyzji w grach komputerowych. To narzÄ™dzie umoÅ¼liwia agentom samodzielne doskonalenie strategii, poprzez interakcjÄ™ z otoczeniem.

## ğŸ“ Algorytm Q-learn i projekt agenta
W projekcie zaimplementowano algorytm Q-learn, ktÃ³ry jest jednym z popularnych algorytmÃ³w uczenia ze wzmacnianiem. Algorytm ten polega na budowaniu funkcji wartoÅ›ci akcji, zwanej rÃ³wnieÅ¼ funkcjÄ… Q, ktÃ³ra okreÅ›la, jak dobre sÄ… rÃ³Å¼ne akcje w danym stanie Å›rodowiska. <br />
Na poczÄ…tku algorytmu tworzona jest tabela Q, w ktÃ³rej przechowywane sÄ… wartoÅ›ci Q dla kaÅ¼dej kombinacji stanu i akcji. W mojej implementacji jest to dwuwymiarowa tabela states wypeÅ‚niona instancjami klasy State. UÅ‚atwia to pobieranie najlepszej akcji i wartoÅ›ci dla kaÅ¼dego stanu.
```java
private State[][] states;
```
```java
public class State
{
    public double [] qValues = {0,0,0,0};

    public int GetBestAction()
    {
        int bestAction = 0;
        double bestValue = -100;

        for(int i=0; i<qValues.length; i++)
        {
            if(qValues[i] > bestValue || (qValues[i] == bestValue && (Math.random() > 0.5f)))
            {
                bestAction=i;
                bestValue=qValues[i];
            }
        }

        return  bestAction;
    }
    public double GetBestValue()
    {
        double bestValue = -100;

        for (double qValue : qValues)
        {
            if (qValue > bestValue) bestValue = qValue;
        }

        return  bestValue;
    }
};
```
KaÅ¼dy agent ma przypisywane te wartoÅ›ci w sposÃ³b losowy. OscylujÄ… one wokÃ³Å‚ pewnych wartoÅ›ci uznanych za standardowe i przynoszÄ…ce zadowalajÄ…ce rezultaty w kaÅ¼dym Å›rodowisku
```java
epsilon = 0.75 + Math.random() * (0.75 - 0.5);
a = 0.05 + Math.random() * (0.5 - 0.05);
y = 0.5 + Math.random() * (0.99 - 0.5 );
```
Agent, bÄ™dÄ…c w danym stanie, wybiera akcjÄ™ do wykonania. WybÃ³r ten moÅ¼e byÄ‡ zgodny z zasadÄ… eksploatacji (wybieranie najlepszej znanej akcji) lub eksploracji (wybieranie losowej akcji w celu poszerzenia wiedzy agenta). NastÄ™pnie agent wykonuje wybranÄ… akcjÄ™, przechodzÄ…c do nowego stanu Å›rodowiska. Po wykonaniu akcji agent aktualizuje wartoÅ›Ä‡ Q zgodnie ze wzorem:

![WzÃ³r Q-learning](gfx/Q-learning-equation.svg)

- **Q(s, a)** to wartoÅ›Ä‡ Q dla stanu \( s \) i akcji \( a \),
- **Î±** to wspÃ³Å‚czynnik uczenia,
- **R** to nagroda za wykonanie akcji,
- **Î³**  to wspÃ³Å‚czynnik dyskontowania przyszÅ‚ych nagrÃ³d,
- **\( s' \) i \( a' \)** to kolejny stan i akcja.

### ğŸ‘¾PodstawÄ… dziaÅ‚ania algorytmu sÄ… trzy zmienne:

- **Epsilon (Ïµ) - Exploration Probability.** <br />
Kontroluje balans pomiÄ™dzy eksploracjÄ… a eksploatacjÄ…. Eksploracja polega na podejmowaniu losowych akcji w celu odkrywania nowych moÅ¼liwoÅ›ci, podczas gdy eksploatacja polega na wybieraniu akcji, ktÃ³re agent uwaÅ¼a za obecnie najlepsze.<br />
Dla maÅ‚ych wartoÅ›ci Ïµ, agent bardziej skupia siÄ™ na eksploatacji, wybierajÄ…c akcje na podstawie obecnej wiedzy. <br />
Dla duÅ¼ych wartoÅ›ci  Ïµ, agent czÄ™Å›ciej eksploruje, co moÅ¼e pomÃ³c w odkryciu lepszych strategii.
- **Alfa (Î±) - Learning Rate** <br />
OkreÅ›la, jak szybko agent aktualizuje swojÄ… wiedzÄ™ (Q-values) w trakcie uczenia siÄ™. WartoÅ›Ä‡  Î± kontroluje, jak bardzo nowe informacje wpÅ‚ywajÄ… na juÅ¼ istniejÄ…ce wartoÅ›ci Q.<br />
Dla maÅ‚ych wartoÅ›ci Î±, agent bardziej kieruje siÄ™ wczeÅ›niejszymi doÅ›wiadczeniami, co sprawia, Å¼e uczenie jest bardziej stabilne, ale wolniejsze. <br />Dla duÅ¼ych wartoÅ›ci Î±, agent szybciej dostosowuje siÄ™ do nowych informacji, ale moÅ¼e byÄ‡ bardziej podatny na szumy w danych.
- **Gamma (Î³) - Discount Factor** <br />
OkreÅ›la, jak bardzo agent zwraca uwagÄ™ na przyszÅ‚e nagrody. WpÅ‚ywa na to, jak bardzo agent jest zorientowany na zdyskontowane nagrody w przyszÅ‚oÅ›ci w porÃ³wnaniu z natychmiastowymi nagrodami.<br />
Dla maÅ‚ych wartoÅ›ci  Î³, agent bardziej skupia siÄ™ na natychmiastowych nagrodach, ignorujÄ…c przyszÅ‚e korzyÅ›ci. <br />
Dla duÅ¼ych wartoÅ›ci Î³, agent bardziej uwzglÄ™dnia zdyskontowane nagrody, co sprawia, Å¼e jest bardziej dÅ‚ugoterminowy.

Agent podejmuje decyzje w poniÅ¼szej metodzie:
```java
public int chooseAction()
    {
        steps++;
        if (Math.random() < epsilon)
        {
            //best
            return states[posX][posY].GetBestAction();
        }
        else
        {
            //random
            return (int)(Math.random()*4);
        }
    }
```
WidaÄ‡ tutaj jak wartoÅ›Ä‡ Ïµ bezpoÅ›rednio wpÅ‚ywa na czas jaki agent poÅ›wiÄ™ca na zwiedzanie labiryntu, w stosunku do Å›lepego podÄ…Å¼ania za nagrodami.

![Hare1](gfx/BabyKicajec0.png)
![Hare2](gfx/BabyKicajec1.png)

## ğŸ“ Generowanie LabiryntÃ³w

Projekt wykorzystuje algorytm "recursive backtracking" do generowania labiryntÃ³w w formie tablicy 2D. Algorytm uÅ¼ywa rekurencji do eksplorowania labiryntu. Punkty sÄ… wybierane losowo, a nastÄ™pnie odwiedzane sÄ…siednie punkty, usuwajÄ…c Å›ciany miÄ™dzy nimi. JeÅ›li dany punkt nie ma dostÄ™pnych sÄ…siadÃ³w, algorytm wraca do poprzedniego punktu (backtrack), kontynuujÄ…c proces.

### ğŸ‘¾Kroki algorytmu:
- Wybierz punkt poczÄ…tkowy
- Wybierz losowÄ… sÄ…siedniÄ… komÃ³rkÄ™. UtwÃ³rz przejÅ›cie tylko wtedy, gdy komÃ³rka ta nie zostaÅ‚a jeszcze odwiedzona
- Powtarzaj proces, aÅ¼ nie bÄ™dzie juÅ¼ dostÄ™pnych sÄ…siednich komÃ³rek
- Rozpocznij cofanie siÄ™ (backtrack) aÅ¼ do momentu, gdy ponownie bÄ™dzie moÅ¼liwe wybranie komÃ³rki
- Algorytm koÅ„czy siÄ™, gdy wrÃ³cisz do punktu poczÄ…tkowego

Wybierz Å›cieÅ¼kÄ™ i idÅº niÄ…, aÅ¼ dotrzesz do Å›lepego zauÅ‚ka. NastÄ™pnie cofnij siÄ™ do momentu, w ktÃ³rym moÅ¼esz zaczÄ…Ä‡ wybieraÄ‡ Å›cieÅ¼ki ponownie. Powtarzaj to, aÅ¼ dotrzesz do celu.

## ğŸ‘©ğŸ½â€ğŸ’»ğŸ§‘ğŸ½â€ğŸ’» Autorzy

Kod: MiÅ‚osz Klim, WydziaÅ‚ Fizyki, Technologie Komputerowe semestr V<br />
Grafika: Wiktoria Bielecka, WydziaÅ‚ Fizyki, Technologie Komputerowe semestr V<br />
Å¹rÃ³dÅ‚a:
- https://aryanab.medium.com/maze-generation-recursive-backtracking-5981bc5cc766
- https://strikingloo.github.io/reinforcement-learning-beginners 
<br />

GrudzieÅ„ 2023